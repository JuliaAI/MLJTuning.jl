const ParameterName=Union{Symbol,Expr}

"""
    RandomSearch(bounded=Distributions.Uniform,
                 positive_unbounded=Distributions.Gamma,
                 others=Normal,
                 rng=Random.GLOBAL_RNG)

Instantiate a random search tuning strategy for searching over
Cartesian hyperparemeter domains.

### Supported ranges:

- A single one-dimensional range (`ParamRange` object) `r`

- A pair of the form `(r, d)`, with `r` as above and where `d` is a
  probability vector of the same length as `r.values`, if `r` is a
  `NominalRange`, and is otherwise: (i) any `Distributions.Univariate`
  *instance*; or (ii) one of the *subtypes* of
  `Distributions.Univariate` listed in the table below, for automatic
  fitting using `Distributions.fit(d, r)` (a distribution whose
  support always lies between `r.lower` and `r.upper`.

- Any pair of the form `(field, s)`, where `field` is the, possibly
  nested, name of a field the model to be tuned, and `s` an arbitrary
  sampler object for that field. (This only means `rand(rng, s)` is defined and
  returns valid values for the field.)

- Any vector of objects of the above form

distribution types  | for fitting to ranges of this type
--------------------|-----------------------------------
`Arcsine`, `Uniform`, `Biweight`, `Cosine`, `Epanechnikov`, `SymTriangularDist`, `Triweight` | bounded
`Gamma`, `InverseGaussian`, `Poisson` | positive
`Normal`, `Logistic`, `LogNormal`, `Cauchy`, `Gumbel`, `Laplace`  | any

`ParamRange` objects are constructed using the `range` method.

### Examples:

    range1 = range(model, :hyper1, lower=1, origin=2, unit=1)

    range2 = [(range(model, :hyper1, lower=1, upper=10), Arcsine),
               range(model, :hyper2, lower=2, upper=4),
              (range(model, :hyper2, lower=2, upper=4), Normal(0, 3)),
               range(model, :hyper3, values=[:ball, :tree], [0.3, 0.7])]

    # uniform sampling of :(atom.λ) from [0, 1] without defining a NumericRange:
    struct MySampler end
    Base.rand(rng::AbstractRNG, ::MySampler) = rand(rng)
    range3 = (:(atom.λ), MySampler())

### Algorithm

Models for evaulation are generated by sampling each range `r` using
`rand(rng, s)` where, `s = sampler(r, d)`. See `sampler` for details. If `d`
is not specified, then sampling is uniform (with replacement) in the
case of a `NominalRange`, and is otherwise given by the defaults
specified by the tuning strategy parameters `bounded`,
`positive_unbounded`, and `other`, depending on the `NumericRange`
type.

See also [`TunedModel`](@ref), [`range`](@ref), [`sampler`](@ref).

"""
mutable struct RandomSearch <: TuningStrategy 
    bounded::Distributions.Univariate
    positive_unbounded::Distributions.Univariate
    others::Distribution.Univariate
    rng::Random.AbstractRNG
end

# Constructor with keywords
function RandomSearch(; bounded=Distributions.Uniform,
                      positive_unbounded=Distributions.Gamma,
                      others=Normal,
                      rng=Random.GLOBAL_RNG)
    _rng = rng isa Integer ? Random.MersenneTwister(rng) : rng
    return RandomSearch(bounded, positive_unbounded, others, rng)
end

isnumeric(::Any) = false
isnumeric(::NumericRange) = true

# function setup(tuning::RandomSearch, model, user_range, verbosity)
#     ranges, distributions = # I AM HERE
#         process_user_random_range(user_range, tuning.distribution, verbosity)
#     distributions = adjusted_distributions(tuning.goal, ranges, distributions)

#     fields = map(r -> r.field, ranges)

#     parameter_scales = scale.(ranges)

#     if tuning.shuffle
#         models = grid(tuning.rng, model, ranges, distributions)
#     else
#         models = grid(model, ranges, distributions)
#     end

#     state = (models=models,
#              fields=fields,
#              parameter_scales=parameter_scales)

#     return state

# end

# MLJTuning.models!(tuning::RandomSearch, model, history::Nothing,
#                   state, verbosity) = state.models
# MLJTuning.models!(tuning::RandomSearch, model, history,
#                   state, verbosity) =
#     state.models[length(history) + 1:end]

# function tuning_report(tuning::RandomSearch, history, state)

#     plotting = plotting_report(state.fields, state.parameter_scales, history)

#     # todo: remove collects?
#     return (history=history, plotting=plotting)

# end

# function default_n(tuning::RandomSearch, user_range)
#     ranges, distributions =
#         process_grid_range(user_range, tuning.distribution, -1)

#     distributions = adjusted_distributions(tuning.goal, ranges, distributions)
#     len(t::Tuple{NumericRange,Integer}) = length(iterator(t[1], t[2]))
#     len(t::Tuple{NominalRange,Integer}) = t[2]
#     return prod(len.(zip(ranges, distributions)))

# end
